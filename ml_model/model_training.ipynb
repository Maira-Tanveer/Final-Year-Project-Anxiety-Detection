{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PD_h6a1-FZtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b26db75-47b7-45ed-ba17-808ec6924445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original columns: ['Unnamed: 0', 'text', 'anxiety', 'all_labels', 'levels', 'Urdu_text']\n",
            "Original shape: (12000, 6)\n",
            "Removed columns: ['Unnamed: 0', 'text']\n",
            "Remaining columns: ['anxiety', 'all_labels', 'levels', 'Urdu_text']\n",
            "New shape: (12000, 4)\n",
            "\n",
            "First 5 rows after removing columns:\n",
            "   anxiety all_labels      levels  \\\n",
            "0        0     Normal  No Anxiety   \n",
            "1        0     Normal  No Anxiety   \n",
            "2        0     Normal  No Anxiety   \n",
            "3        0     Normal  No Anxiety   \n",
            "4        0     Normal  No Anxiety   \n",
            "\n",
            "                                           Urdu_text  \n",
            "0           مجھے آپ کا ٹویٹر نہیں ملا، جواب خراب ہے۔  \n",
            "1  سب کو شب بخیر اور جیرڈ، کبھی بھی کائٹ پف میں چ...  \n",
            "2  ایپل میوزک فروخت ہو رہا ہے۔ 3 ماہ کی ایکٹیویشن...  \n",
            "3  تقریب ابھی تک شروع نہیں ہوئی۔ محترمہ، صرف ذکر ...  \n",
            "4             میں واقعی امید کرتا ہوں کہ سردی نہ ہو۔  \n",
            "\n",
            "Updated dataset saved to updated_anxiety_dataset.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the file (works for both Excel and CSV)\n",
        "file_path = 'anxiety_dataset.xlsx'  # Replace with your file path\n",
        "# For Excel\n",
        "if file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
        "    df = pd.read_excel(file_path)\n",
        "# For CSV\n",
        "elif file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path)\n",
        "else:\n",
        "    raise ValueError(\"File format not supported. Please use Excel (.xlsx, .xls) or CSV (.csv).\")\n",
        "\n",
        "# Print original columns and shape\n",
        "print(\"Original columns:\", df.columns.tolist())\n",
        "print(\"Original shape:\", df.shape)\n",
        "\n",
        "# Create a list of columns to remove\n",
        "columns_to_remove = []\n",
        "\n",
        "# Check if 'Unnamed: 0' column exists\n",
        "if 'Unnamed: 0' in df.columns:\n",
        "    columns_to_remove.append('Unnamed: 0')\n",
        "\n",
        "# Check if 'text' column exists\n",
        "if 'text' in df.columns:\n",
        "    columns_to_remove.append('text')\n",
        "\n",
        "# Remove the specified columns if they exist\n",
        "if columns_to_remove:\n",
        "    df = df.drop(columns=columns_to_remove)\n",
        "    print(f\"Removed columns: {columns_to_remove}\")\n",
        "else:\n",
        "    print(\"None of the specified columns found in the dataset.\")\n",
        "\n",
        "# Print remaining columns and shape\n",
        "print(\"Remaining columns:\", df.columns.tolist())\n",
        "print(\"New shape:\", df.shape)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(\"\\nFirst 5 rows after removing columns:\")\n",
        "print(df.head())\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file = 'updated_' + os.path.basename(file_path)\n",
        "if file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
        "    df.to_excel(output_file, index=False)\n",
        "else:  # CSV\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\nUpdated dataset saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**updated code**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JdtLWqz27o91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " code without hyperparameter tunning\n"
      ],
      "metadata": {
        "id": "vaC5ZkzA266X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Urdu Anxiety Level Prediction Model - Training and Prediction\n",
        "-------------------------------------------------------------\n",
        "This script performs the essential steps to train and test an anxiety prediction model:\n",
        "1. Data preprocessing and cleaning for Urdu text\n",
        "2. Feature extraction using TF-IDF\n",
        "3. Model training and evaluation\n",
        "4. User input prediction for anxiety, all_labels, and levels\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "\n",
        "# NLP and ML libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ===================================================================================================\n",
        "# PART 1: DATA LOADING AND EXPLORATION\n",
        "# ===================================================================================================\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    Load and explore the dataset\n",
        "    \"\"\"\n",
        "    print(\"Starting to load data from:\", filepath)\n",
        "\n",
        "    try:\n",
        "        # Load data - Excel doesn't need the encoding parameter\n",
        "        df = pd.read_excel(filepath)\n",
        "\n",
        "        # Basic dataset information\n",
        "        print(f\"Dataset shape: {df.shape}\")\n",
        "        print(\"\\nColumn information:\")\n",
        "        print(df.info())\n",
        "\n",
        "        # Check for missing values\n",
        "        print(\"\\nMissing values:\")\n",
        "        print(df.isnull().sum())\n",
        "\n",
        "        # Target variable distribution\n",
        "        print(\"\\nDistribution of anxiety labels:\")\n",
        "        print(df['anxiety'].value_counts())\n",
        "        print(\"\\nDistribution of all_labels:\")\n",
        "        print(df['all_labels'].value_counts())\n",
        "        print(\"\\nDistribution of levels:\")\n",
        "        print(df['levels'].value_counts())\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {str(e)}\")\n",
        "        print(\"Trying with engine='openpyxl'...\")\n",
        "        try:\n",
        "            # Try with openpyxl engine\n",
        "            df = pd.read_excel(filepath, engine='openpyxl')\n",
        "            print(\"Successfully loaded with openpyxl engine!\")\n",
        "\n",
        "            # Basic dataset information\n",
        "            print(f\"Dataset shape: {df.shape}\")\n",
        "            print(\"\\nColumn information:\")\n",
        "            print(df.info())\n",
        "\n",
        "            # Check for missing values\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(df.isnull().sum())\n",
        "\n",
        "            # Target variable distribution\n",
        "            print(\"\\nDistribution of anxiety labels:\")\n",
        "            print(df['anxiety'].value_counts())\n",
        "            print(\"\\nDistribution of all_labels:\")\n",
        "            print(df['all_labels'].value_counts())\n",
        "            print(\"\\nDistribution of levels:\")\n",
        "            print(df['levels'].value_counts())\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading with openpyxl: {str(e2)}\")\n",
        "            print(\"Please check your file path and ensure the file is a valid Excel file.\")\n",
        "            raise\n",
        "\n",
        "# ===================================================================================================\n",
        "# PART 2: TEXT PREPROCESSING FOR URDU\n",
        "# ===================================================================================================\n",
        "class UrduTextPreprocessor:\n",
        "    def __init__(self):\n",
        "        # Common Urdu stopwords\n",
        "        self.urdu_stopwords = set([\n",
        "            'کے', 'کا', 'کی', 'میں', 'سے', 'اور', 'ہے', 'کو', 'نے', 'پر', 'ہیں', 'کہ',\n",
        "            'تھا', 'تھی', 'تھے', 'ہوں', 'ہوا', 'ہوئی', 'ہوئے', 'تو', 'اس', 'وہ', 'ان',\n",
        "            'تھیں', 'یہ', 'رہا', 'رہی', 'رہے', 'ہوتا', 'ہوتی', 'ہوتے', 'گا', 'گی', 'گے',\n",
        "            'کر', 'کرے', 'کرتا', 'کرتی', 'کرتے', 'کیا', 'ہوگا', 'ہوگی', 'ہوگے', 'ہوگیا',\n",
        "            'ہوگئی', 'ہوگئے', 'ہوگئیں', 'بھی', 'جو', 'لیے', 'بہت', 'پھر', 'گیا', 'گئی',\n",
        "            'گئے', 'گئیں', 'وغیرہ', 'والا', 'والی', 'والے', 'مگر', 'لیکن', 'جب', 'تب',\n",
        "            'اب', 'اگر', 'تاکہ', 'جبکہ'\n",
        "        ])\n",
        "\n",
        "        # Basic normalization mapping for chat/slang\n",
        "        self.normalization_map = {\n",
        "            'kya': 'کیا',\n",
        "            'hai': 'ہے',\n",
        "            'nhi': 'نہیں',\n",
        "            'mjy': 'مجھے',\n",
        "            'ap': 'آپ',\n",
        "            'aap': 'آپ',\n",
        "            'tha': 'تھا',\n",
        "            'ho': 'ہو',\n",
        "            'yr': 'یار',\n",
        "            'maine': 'میں نے',\n",
        "            'mene': 'میں نے',\n",
        "            # Add more as needed\n",
        "        }\n",
        "\n",
        "        # Simple lemmatization mapping (just a few examples, would need a proper Urdu lemmatizer)\n",
        "        self.lemma_map = {\n",
        "            'چلی': 'چل',\n",
        "            'چلیں': 'چل',\n",
        "            'چلا': 'چل',\n",
        "            'جاتا': 'جا',\n",
        "            'جاتی': 'جا',\n",
        "            'جاتے': 'جا',\n",
        "            'کرتا': 'کر',\n",
        "            'کرتی': 'کر',\n",
        "            'کرتے': 'کر',\n",
        "            'کرنا': 'کر',\n",
        "            'کرنی': 'کر',\n",
        "            'کرنے': 'کر',\n",
        "            # Add more as needed\n",
        "        }\n",
        "\n",
        "    def remove_urls(self, text):\n",
        "        \"\"\"Remove URLs from text\"\"\"\n",
        "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        return url_pattern.sub(r'', text)\n",
        "\n",
        "    def remove_emojis(self, text):\n",
        "        \"\"\"Remove emojis from text\"\"\"\n",
        "        emoji_pattern = re.compile(\n",
        "            \"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "            u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes\n",
        "            u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "            u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "            u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "            u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "            u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE\n",
        "        )\n",
        "        return emoji_pattern.sub(r'', text)\n",
        "\n",
        "    def remove_punctuation_and_numbers(self, text):\n",
        "        \"\"\"Remove punctuation and numbers\"\"\"\n",
        "        # Remove Latin and Arabic/Urdu punctuation and numbers\n",
        "        text = re.sub(r'[.,;:!\\'\\\"()[\\]{}@#$%^&*+_=|<>/?\\d]+', ' ', text)\n",
        "        text = re.sub(r'[،؛؟٬٫٪؍؎''‛\"\"„‟]+', ' ', text)  # Urdu/Arabic punctuation\n",
        "        text = re.sub(r'[۰-۹]+', ' ', text)  # Urdu/Persian numbers\n",
        "        text = re.sub(r'[٠-٩]+', ' ', text)  # Arabic numbers\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize Urdu text into words\"\"\"\n",
        "        # Simple space-based tokenization, can be enhanced with a proper Urdu tokenizer\n",
        "        return text.split()\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        \"\"\"Normalize chat/slang Urdu\"\"\"\n",
        "        words = text.split()\n",
        "        normalized = []\n",
        "        for word in words:\n",
        "            word_lower = word.lower()\n",
        "            if word_lower in self.normalization_map:\n",
        "                normalized.append(self.normalization_map[word_lower])\n",
        "            else:\n",
        "                normalized.append(word)\n",
        "        return ' '.join(normalized)\n",
        "\n",
        "    def remove_stopwords(self, text):\n",
        "        \"\"\"Remove Urdu stopwords\"\"\"\n",
        "        return ' '.join([word for word in text.split() if word not in self.urdu_stopwords])\n",
        "\n",
        "    def lemmatize(self, text):\n",
        "        \"\"\"Simple lemmatization for Urdu based on mapping\"\"\"\n",
        "        words = text.split()\n",
        "        lemmatized = []\n",
        "        for word in words:\n",
        "            if word in self.lemma_map:\n",
        "                lemmatized.append(self.lemma_map[word])\n",
        "            else:\n",
        "                lemmatized.append(word)\n",
        "        return ' '.join(lemmatized)\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Complete preprocessing pipeline for Urdu text\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text)\n",
        "        text = self.remove_urls(text)\n",
        "        text = self.remove_emojis(text)\n",
        "        text = self.remove_punctuation_and_numbers(text)\n",
        "        text = self.normalize_text(text)\n",
        "        text = self.remove_stopwords(text)\n",
        "        text = self.lemmatize(text)\n",
        "        # Remove extra spaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "# ===================================================================================================\n",
        "# PART 3: FEATURE ENGINEERING AND DATA PREPARATION\n",
        "# ===================================================================================================\n",
        "def prepare_data(df, preprocessor):\n",
        "    \"\"\"\n",
        "    Prepare data for modeling:\n",
        "    1. Preprocess text\n",
        "    2. Encode labels\n",
        "    3. Split data\n",
        "    \"\"\"\n",
        "    print(\"Preparing data for modeling...\")\n",
        "\n",
        "    # Preprocess Urdu text\n",
        "    print(\"Preprocessing Urdu text...\")\n",
        "    df['cleaned_text'] = df['Urdu_text'].apply(preprocessor.preprocess)\n",
        "\n",
        "    # Check if we have empty strings after preprocessing\n",
        "    empty_rows = df[df['cleaned_text'] == ''].shape[0]\n",
        "    print(f\"Rows with empty text after preprocessing: {empty_rows}\")\n",
        "\n",
        "    # Remove rows with empty text if any\n",
        "    if empty_rows > 0:\n",
        "        df = df[df['cleaned_text'] != ''].reset_index(drop=True)\n",
        "        print(f\"Removed {empty_rows} rows with empty text.\")\n",
        "\n",
        "    # Create TF-IDF features\n",
        "    print(\"Creating TF-IDF features...\")\n",
        "    tfidf_vectorizer = TfidfVectorizer(\n",
        "        min_df=2,             # Minimum document frequency\n",
        "        max_df=0.95,          # Maximum document frequency\n",
        "        ngram_range=(1, 2),   # Use unigrams and bigrams\n",
        "        max_features=5000     # Limit features to 5000\n",
        "    )\n",
        "    X = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
        "    print(f\"TF-IDF features shape: {X.shape}\")\n",
        "\n",
        "    # Encode labels\n",
        "    print(\"Encoding labels...\")\n",
        "    # For binary classification (anxiety)\n",
        "    y_binary = df['anxiety'].values\n",
        "\n",
        "    # For multi-class classification (all_labels and levels)\n",
        "    print(\"Checking class frequencies in all_labels...\")\n",
        "    all_labels_counts = df['all_labels'].value_counts()\n",
        "    print(all_labels_counts)\n",
        "\n",
        "    # Filter out rare classes with only 1 instance for all_labels\n",
        "    rare_classes = all_labels_counts[all_labels_counts == 1].index.tolist()\n",
        "    if rare_classes:\n",
        "        print(f\"Found rare classes with only 1 instance: {rare_classes}\")\n",
        "        print(\"Filtering out rare classes for all_labels before encoding\")\n",
        "        df_filtered_all_labels = df[~df['all_labels'].isin(rare_classes)].copy()\n",
        "        print(f\"Removed {len(df) - len(df_filtered_all_labels)} rows with rare classes\")\n",
        "    else:\n",
        "        df_filtered_all_labels = df.copy()\n",
        "\n",
        "    label_encoder_all_labels = LabelEncoder()\n",
        "    y_all_labels = label_encoder_all_labels.fit_transform(df_filtered_all_labels['all_labels'])\n",
        "\n",
        "    print(\"Checking class frequencies in levels...\")\n",
        "    levels_counts = df['levels'].value_counts()\n",
        "    print(levels_counts)\n",
        "\n",
        "    # Filter out rare classes with only 1 instance for levels\n",
        "    rare_levels = levels_counts[levels_counts == 1].index.tolist()\n",
        "    if rare_levels:\n",
        "        print(f\"Found rare levels with only 1 instance: {rare_levels}\")\n",
        "        print(\"Filtering out rare levels before encoding\")\n",
        "        df_filtered_levels = df[~df['levels'].isin(rare_levels)].copy()\n",
        "        print(f\"Removed {len(df) - len(df_filtered_levels)} rows with rare levels\")\n",
        "    else:\n",
        "        df_filtered_levels = df.copy()\n",
        "\n",
        "    label_encoder_levels = LabelEncoder()\n",
        "    y_levels = label_encoder_levels.fit_transform(df_filtered_levels['levels'])\n",
        "\n",
        "    print(f\"Binary target distribution: {Counter(y_binary)}\")\n",
        "    print(f\"All labels target distribution: {Counter(y_all_labels)}\")\n",
        "    print(f\"Levels target distribution: {Counter(y_levels)}\")\n",
        "\n",
        "    # Split data: 70% training, 20% testing, 10% validation for binary classification\n",
        "    print(\"\\nSplitting data for binary classification...\")\n",
        "    X_temp, X_test, y_binary_temp, y_binary_test = train_test_split(\n",
        "        X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_binary_train, y_binary_val = train_test_split(\n",
        "        X_temp, y_binary_temp, test_size=0.125, random_state=42, stratify=y_binary_temp\n",
        "    )\n",
        "\n",
        "    # For all_labels classification, use the filtered dataset\n",
        "    print(\"\\nSplitting data for all_labels classification...\")\n",
        "    if len(df) != len(df_filtered_all_labels):\n",
        "        # Use indices to extract the proper subset of X for all_labels\n",
        "        all_labels_indices = df_filtered_all_labels.index\n",
        "        X_all_labels = X[all_labels_indices]\n",
        "\n",
        "        X_all_labels_temp, X_all_labels_test, y_all_labels_temp, y_all_labels_test = train_test_split(\n",
        "            X_all_labels, y_all_labels, test_size=0.2, random_state=42, stratify=y_all_labels\n",
        "        )\n",
        "\n",
        "        X_all_labels_train, X_all_labels_val, y_all_labels_train, y_all_labels_val = train_test_split(\n",
        "            X_all_labels_temp, y_all_labels_temp, test_size=0.125, random_state=42, stratify=y_all_labels_temp\n",
        "        )\n",
        "    else:\n",
        "        # If no filtering was done, use the same splits as binary\n",
        "        X_all_labels_temp, X_all_labels_test, y_all_labels_temp, y_all_labels_test = train_test_split(\n",
        "            X, y_all_labels, test_size=0.2, random_state=42, stratify=y_all_labels\n",
        "        )\n",
        "\n",
        "        X_all_labels_train, X_all_labels_val, y_all_labels_train, y_all_labels_val = train_test_split(\n",
        "            X_all_labels_temp, y_all_labels_temp, test_size=0.125, random_state=42, stratify=y_all_labels_temp\n",
        "        )\n",
        "\n",
        "    # For levels classification, use the filtered dataset\n",
        "    print(\"\\nSplitting data for levels classification...\")\n",
        "    if len(df) != len(df_filtered_levels):\n",
        "        # Use indices to extract the proper subset of X for levels\n",
        "        levels_indices = df_filtered_levels.index\n",
        "        X_levels = X[levels_indices]\n",
        "\n",
        "        X_levels_temp, X_levels_test, y_levels_temp, y_levels_test = train_test_split(\n",
        "            X_levels, y_levels, test_size=0.2, random_state=42, stratify=y_levels\n",
        "        )\n",
        "\n",
        "        X_levels_train, X_levels_val, y_levels_train, y_levels_val = train_test_split(\n",
        "            X_levels_temp, y_levels_temp, test_size=0.125, random_state=42, stratify=y_levels_temp\n",
        "        )\n",
        "    else:\n",
        "        # If no filtering was done, use the same splits as binary\n",
        "        X_levels_temp, X_levels_test, y_levels_temp, y_levels_test = train_test_split(\n",
        "            X, y_levels, test_size=0.2, random_state=42, stratify=y_levels\n",
        "        )\n",
        "\n",
        "        X_levels_train, X_levels_val, y_levels_train, y_levels_val = train_test_split(\n",
        "            X_levels_temp, y_levels_temp, test_size=0.125, random_state=42, stratify=y_levels_temp\n",
        "        )\n",
        "\n",
        "    print(f\"Binary - Train set size: {X_train.shape[0]}\")\n",
        "    print(f\"Binary - Validation set size: {X_val.shape[0]}\")\n",
        "    print(f\"Binary - Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "    print(f\"All Labels - Train set size: {X_all_labels_train.shape[0]}\")\n",
        "    print(f\"All Labels - Validation set size: {X_all_labels_val.shape[0]}\")\n",
        "    print(f\"All Labels - Test set size: {X_all_labels_test.shape[0]}\")\n",
        "\n",
        "    print(f\"Levels - Train set size: {X_levels_train.shape[0]}\")\n",
        "    print(f\"Levels - Validation set size: {X_levels_val.shape[0]}\")\n",
        "    print(f\"Levels - Test set size: {X_levels_test.shape[0]}\")\n",
        "\n",
        "    targets = {\n",
        "        'binary': {\n",
        "            'train': y_binary_train,\n",
        "            'val': y_binary_val,\n",
        "            'test': y_binary_test\n",
        "        },\n",
        "        'all_labels': {\n",
        "            'train': y_all_labels_train,\n",
        "            'val': y_all_labels_val,\n",
        "            'test': y_all_labels_test\n",
        "        },\n",
        "        'levels': {\n",
        "            'train': y_levels_train,\n",
        "            'val': y_levels_val,\n",
        "            'test': y_levels_test\n",
        "        }\n",
        "    }\n",
        "\n",
        "    encoders = {\n",
        "        'tfidf': tfidf_vectorizer,\n",
        "        'all_labels': label_encoder_all_labels,\n",
        "        'levels': label_encoder_levels\n",
        "    }\n",
        "\n",
        "    data_splits = {\n",
        "        'binary': {\n",
        "            'X_train': X_train,\n",
        "            'X_val': X_val,\n",
        "            'X_test': X_test\n",
        "        },\n",
        "        'all_labels': {\n",
        "            'X_train': X_all_labels_train,\n",
        "            'X_val': X_all_labels_val,\n",
        "            'X_test': X_all_labels_test\n",
        "        },\n",
        "        'levels': {\n",
        "            'X_train': X_levels_train,\n",
        "            'X_val': X_levels_val,\n",
        "            'X_test': X_levels_test\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Store original label values for interpretation\n",
        "    original_labels = {\n",
        "        'all_labels': df_filtered_all_labels['all_labels'].unique(),\n",
        "        'levels': df_filtered_levels['levels'].unique()\n",
        "    }\n",
        "\n",
        "    return data_splits, targets, encoders, original_labels\n",
        "\n",
        "# ===================================================================================================\n",
        "# PART 4: MODEL TRAINING AND EVALUATION\n",
        "# ===================================================================================================\n",
        "def train_evaluate_models(data_splits, targets, target_type='binary'):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple classifiers for the specified target type\n",
        "    \"\"\"\n",
        "    print(f\"\\nTraining and evaluating models for {target_type} classification...\")\n",
        "\n",
        "    # Get the appropriate data splits for the target type\n",
        "    X_train = data_splits[target_type]['X_train']\n",
        "    X_val = data_splits[target_type]['X_val']\n",
        "    X_test = data_splits[target_type]['X_test']\n",
        "\n",
        "    y_train = targets[target_type]['train']\n",
        "    y_val = targets[target_type]['val']\n",
        "    y_test = targets[target_type]['test']\n",
        "\n",
        "    # Define the models to be trained\n",
        "    models = {\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'Multinomial NB': MultinomialNB(),\n",
        "        'Support Vector Machine': SVC(kernel='linear', probability=True, random_state=42),\n",
        "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "        'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "        'LightGBM': lgb.LGBMClassifier(random_state=42)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on training, validation, and test sets\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy on all sets\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "        # Calculate F1 score for test set\n",
        "        test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "        print(f\"{name} Results:\")\n",
        "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"Test F1 Score (weighted): {test_f1:.4f}\")\n",
        "\n",
        "        # Print confusion matrix for test set\n",
        "        print(\"\\nConfusion Matrix (Test Set):\")\n",
        "        cm = confusion_matrix(y_test, y_test_pred)\n",
        "        print(cm)\n",
        "\n",
        "        # Generate classification report for test set\n",
        "        print(\"\\nClassification Report (Test Set):\")\n",
        "        cr = classification_report(y_test, y_test_pred)\n",
        "        print(cr)\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'train_accuracy': train_accuracy,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'test_f1': test_f1\n",
        "        }\n",
        "\n",
        "    # Identify the best models based on validation accuracy\n",
        "    best_models = sorted(results.items(), key=lambda x: x[1]['val_accuracy'], reverse=True)\n",
        "    print(\"\\nModels ranked by validation accuracy:\")\n",
        "    for i, (name, result) in enumerate(best_models):\n",
        "        print(f\"{i+1}. {name}: {result['val_accuracy']:.4f}\")\n",
        "\n",
        "    # Return the results dictionary\n",
        "    return results\n",
        "\n",
        "# ===================================================================================================\n",
        "# PART 5: MODEL SAVING\n",
        "# ===================================================================================================\n",
        "def save_best_model(results, encoders, target_type='binary'):\n",
        "    \"\"\"\n",
        "    Save the best model and required encoders\n",
        "    \"\"\"\n",
        "    print(\"\\nSaving the best model and encoders...\")\n",
        "\n",
        "    # Choose the best model based on validation accuracy\n",
        "    best_model_name = max(results.items(), key=lambda x: x[1]['val_accuracy'])[0]\n",
        "    best_model = results[best_model_name]['model']\n",
        "    best_model_accuracy = results[best_model_name]['val_accuracy']\n",
        "\n",
        "    print(f\"Best model: {best_model_name} with validation accuracy: {best_model_accuracy:.4f}\")\n",
        "\n",
        "    # Save model and encoders\n",
        "    model_filename = f'model_{target_type}.pkl'\n",
        "    joblib.dump(best_model, model_filename)\n",
        "\n",
        "    tfidf_filename = f'tfidf_vectorizer_{target_type}.pkl'\n",
        "    joblib.dump(encoders['tfidf'], tfidf_filename)\n",
        "\n",
        "    if target_type == 'all_labels':\n",
        "        encoder_filename = f'label_encoder_all_labels.pkl'\n",
        "        joblib.dump(encoders['all_labels'], encoder_filename)\n",
        "    elif target_type == 'levels':\n",
        "        encoder_filename = f'label_encoder_levels.pkl'\n",
        "        joblib.dump(encoders['levels'], encoder_filename)\n",
        "\n",
        "    # Save model info\n",
        "    model_info = {\n",
        "        'model_name': best_model_name,\n",
        "        'target_type': target_type,\n",
        "        'accuracy': best_model_accuracy,\n",
        "        'saved_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "    info_filename = f'model_info_{target_type}.json'\n",
        "    with open(info_filename, 'w') as f:\n",
        "        import json\n",
        "        json.dump(model_info, f, indent=4)\n",
        "\n",
        "    print(f\"Model and encoders for {target_type} saved successfully!\")\n",
        "\n",
        "    return best_model, encoders\n",
        "\n",
        "# ===================================================================================================\n",
        "# PART 6: PREDICTION FUNCTION\n",
        "# ===================================================================================================\n",
        "def predict_anxiety(text, preprocessor, models, encoders, original_labels):\n",
        "    \"\"\"\n",
        "    Predict anxiety, all_labels, and levels for a given text input\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PREDICTION RESULTS FOR USER INPUT TEXT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Preprocess the input text\n",
        "    cleaned_text = preprocessor.preprocess(text)\n",
        "    print(f\"\\nOriginal Text: {text}\")\n",
        "    print(f\"Preprocessed Text: {cleaned_text}\")\n",
        "\n",
        "    # Vectorize the cleaned text using the saved TF-IDF vectorizers\n",
        "    X_binary = encoders['tfidf'].transform([cleaned_text])\n",
        "\n",
        "    # Make predictions using the saved models\n",
        "    # 1. Binary anxiety prediction (0 or 1)\n",
        "    binary_model = models['binary']\n",
        "    binary_prediction = binary_model.predict(X_binary)[0]\n",
        "    binary_proba = binary_model.predict_proba(X_binary)[0]\n",
        "\n",
        "    print(\"\\n1. ANXIETY PREDICTION (Binary)\")\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Prediction: {'Anxious' if binary_prediction == 1 else 'Not Anxious'}\")\n",
        "    print(f\"Confidence: {max(binary_proba)*100:.2f}%\")\n",
        "\n",
        "    # 2. All_labels prediction\n",
        "    all_labels_model = models['all_labels']\n",
        "    all_labels_prediction = all_labels_model.predict(X_binary)[0]\n",
        "    all_labels_proba = all_labels_model.predict_proba(X_binary)[0]\n",
        "\n",
        "    # Convert encoded prediction back to original label\n",
        "    original_all_labels = encoders['all_labels'].inverse_transform([all_labels_prediction])[0]\n",
        "\n",
        "    print(\"\\n2. ALL LABELS PREDICTION\")\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Prediction: {original_all_labels}\")\n",
        "    print(f\"Confidence: {max(all_labels_proba)*100:.2f}%\")\n",
        "\n",
        "    # 3. Levels prediction\n",
        "    levels_model = models['levels']\n",
        "    levels_prediction = levels_model.predict(X_binary)[0]\n",
        "    levels_proba = levels_model.predict_proba(X_binary)[0]\n",
        "\n",
        "    # Convert encoded prediction back to original label\n",
        "    original_levels = encoders['levels'].inverse_transform([levels_prediction])[0]\n",
        "\n",
        "    print(\"\\n3. LEVELS PREDICTION\")\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Prediction: {original_levels}\")\n",
        "    print(f\"Confidence: {max(levels_proba)*100:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    return {\n",
        "        'anxiety': {'prediction': 'Anxious' if binary_prediction == 1 else 'Not Anxious', 'confidence': max(binary_proba)*100},\n",
        "        'all_labels': {'prediction': original_all_labels, 'confidence': max(all_labels_proba)*100},\n",
        "        'levels': {'prediction': original_levels, 'confidence': max(levels_proba)*100}\n",
        "    }\n",
        "\n",
        "# ===================================================================================================\n",
        "# PART 7: MAIN FUNCTION TO RUN THE PIPELINE\n",
        "# ===================================================================================================\n",
        "# ===================================================================================================\n",
        "# PART 7: MAIN FUNCTION TO RUN THE PIPELINE\n",
        "# ===================================================================================================\n",
        "def main(filepath):\n",
        "    \"\"\"\n",
        "    Run the end-to-end pipeline for all three target types and enable prediction\n",
        "    \"\"\"\n",
        "    print(\"Starting Urdu Anxiety Prediction Pipeline...\")\n",
        "\n",
        "    # 1. Load and explore data\n",
        "    df = load_data(filepath)\n",
        "\n",
        "    # 2. Create preprocessor\n",
        "    preprocessor = UrduTextPreprocessor()\n",
        "\n",
        "    # 3. Prepare data for all target types\n",
        "    data_splits, targets, encoders, original_labels = prepare_data(df, preprocessor)\n",
        "\n",
        "    # 4. Train and evaluate models for each target type\n",
        "    binary_results = train_evaluate_models(data_splits, targets, target_type='binary')\n",
        "    all_labels_results = train_evaluate_models(data_splits, targets, target_type='all_labels')\n",
        "    levels_results = train_evaluate_models(data_splits, targets, target_type='levels')\n",
        "\n",
        "    # 5. Save best models for each target type\n",
        "    binary_best_model, _ = save_best_model(binary_results, encoders, target_type='binary')\n",
        "    all_labels_best_model, _ = save_best_model(all_labels_results, encoders, target_type='all_labels')\n",
        "    levels_best_model, _ = save_best_model(levels_results, encoders, target_type='levels')\n",
        "\n",
        "    # Organize models and encoders for prediction\n",
        "    best_models = {\n",
        "        'binary': binary_best_model,\n",
        "        'all_labels': all_labels_best_model,\n",
        "        'levels': levels_best_model\n",
        "    }\n",
        "\n",
        "    print(\"\\nAll models trained successfully!\")\n",
        "\n",
        "    # 6. User input prediction\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"URDU ANXIETY PREDICTION SYSTEM\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"\\nEnter Urdu text to predict anxiety level (or type 'exit' to quit):\")\n",
        "        user_text = input(\">> \")\n",
        "\n",
        "        if user_text.lower() == 'exit':\n",
        "            print(\"Exiting prediction system.\")\n",
        "            break\n",
        "\n",
        "        # Make predictions using all three models\n",
        "        predictions = predict_anxiety(user_text, preprocessor, best_models, encoders, original_labels)\n",
        "\n",
        "        # Ask if user wants to continue\n",
        "        continue_choice = input(\"\\nDo you want to make another prediction? (y/n): \")\n",
        "        if continue_choice.lower() != 'y':\n",
        "            print(\"Thank you for using the Urdu Anxiety Prediction System!\")\n",
        "            break\n",
        "\n",
        "    return best_models, encoders\n",
        "\n",
        "# ===================================================================================================\n",
        "# EXECUTE THE PIPELINE\n",
        "# ===================================================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify your dataset file path here\n",
        "    filepath = \"updated_anxiety_dataset.xlsx\"  # CHANGE THIS TO YOUR ACTUAL DATASET FILE PATH\n",
        "\n",
        "    # Run the pipeline for all target types and enable prediction\n",
        "    best_models, prediction_encoders = main(filepath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub7-H_nX26GR",
        "outputId": "814b6c9e-7a4c-4167-8877-c1017a1abfa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Urdu Anxiety Prediction Pipeline...\n",
            "Starting to load data from: updated_anxiety_dataset.xlsx\n",
            "Dataset shape: (12000, 4)\n",
            "\n",
            "Column information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12000 entries, 0 to 11999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   anxiety     12000 non-null  int64 \n",
            " 1   all_labels  12000 non-null  object\n",
            " 2   levels      12000 non-null  object\n",
            " 3   Urdu_text   12000 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 375.1+ KB\n",
            "None\n",
            "\n",
            "Missing values:\n",
            "anxiety       0\n",
            "all_labels    0\n",
            "levels        0\n",
            "Urdu_text     0\n",
            "dtype: int64\n",
            "\n",
            "Distribution of anxiety labels:\n",
            "anxiety\n",
            "1    7270\n",
            "0    4730\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of all_labels:\n",
            "all_labels\n",
            "Normal                 4730\n",
            "['agoraphobia']        2578\n",
            "['panic']              1513\n",
            "['socialanxiety']      1293\n",
            "['general']            1177\n",
            "['phobia']              367\n",
            "['selectivemutism']     341\n",
            "['phobia'                 1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of levels:\n",
            "levels\n",
            "No Anxiety          4730\n",
            "Severe Anxiety      2576\n",
            "Moderate Anxiety    1888\n",
            "Panic Anxiety       1513\n",
            "Mild Anxiety        1293\n",
            "Name: count, dtype: int64\n",
            "Preparing data for modeling...\n",
            "Preprocessing Urdu text...\n",
            "Rows with empty text after preprocessing: 0\n",
            "Creating TF-IDF features...\n",
            "TF-IDF features shape: (12000, 5000)\n",
            "Encoding labels...\n",
            "Checking class frequencies in all_labels...\n",
            "all_labels\n",
            "Normal                 4730\n",
            "['agoraphobia']        2578\n",
            "['panic']              1513\n",
            "['socialanxiety']      1293\n",
            "['general']            1177\n",
            "['phobia']              367\n",
            "['selectivemutism']     341\n",
            "['phobia'                 1\n",
            "Name: count, dtype: int64\n",
            "Found rare classes with only 1 instance: [\"['phobia'\"]\n",
            "Filtering out rare classes for all_labels before encoding\n",
            "Removed 1 rows with rare classes\n",
            "Checking class frequencies in levels...\n",
            "levels\n",
            "No Anxiety          4730\n",
            "Severe Anxiety      2576\n",
            "Moderate Anxiety    1888\n",
            "Panic Anxiety       1513\n",
            "Mild Anxiety        1293\n",
            "Name: count, dtype: int64\n",
            "Binary target distribution: Counter({np.int64(1): 7270, np.int64(0): 4730})\n",
            "All labels target distribution: Counter({np.int64(0): 4730, np.int64(1): 2578, np.int64(3): 1513, np.int64(6): 1293, np.int64(2): 1177, np.int64(4): 367, np.int64(5): 341})\n",
            "Levels target distribution: Counter({np.int64(2): 4730, np.int64(4): 2576, np.int64(1): 1888, np.int64(3): 1513, np.int64(0): 1293})\n",
            "\n",
            "Splitting data for binary classification...\n",
            "\n",
            "Splitting data for all_labels classification...\n",
            "\n",
            "Splitting data for levels classification...\n",
            "Binary - Train set size: 8400\n",
            "Binary - Validation set size: 1200\n",
            "Binary - Test set size: 2400\n",
            "All Labels - Train set size: 8399\n",
            "All Labels - Validation set size: 1200\n",
            "All Labels - Test set size: 2400\n",
            "Levels - Train set size: 8400\n",
            "Levels - Validation set size: 1200\n",
            "Levels - Test set size: 2400\n",
            "\n",
            "Training and evaluating models for binary classification...\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "Training Accuracy: 0.9998\n",
            "Validation Accuracy: 0.8300\n",
            "Test Accuracy: 0.8008\n",
            "Test F1 Score (weighted): 0.7990\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 665  281]\n",
            " [ 197 1257]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.70      0.74       946\n",
            "           1       0.82      0.86      0.84      1454\n",
            "\n",
            "    accuracy                           0.80      2400\n",
            "   macro avg       0.79      0.78      0.79      2400\n",
            "weighted avg       0.80      0.80      0.80      2400\n",
            "\n",
            "\n",
            "Training Multinomial NB...\n",
            "Multinomial NB Results:\n",
            "Training Accuracy: 0.8227\n",
            "Validation Accuracy: 0.7942\n",
            "Test Accuracy: 0.7833\n",
            "Test F1 Score (weighted): 0.7793\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 610  336]\n",
            " [ 184 1270]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.64      0.70       946\n",
            "           1       0.79      0.87      0.83      1454\n",
            "\n",
            "    accuracy                           0.78      2400\n",
            "   macro avg       0.78      0.76      0.77      2400\n",
            "weighted avg       0.78      0.78      0.78      2400\n",
            "\n",
            "\n",
            "Training Support Vector Machine...\n",
            "Support Vector Machine Results:\n",
            "Training Accuracy: 0.8987\n",
            "Validation Accuracy: 0.8233\n",
            "Test Accuracy: 0.8092\n",
            "Test F1 Score (weighted): 0.8092\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 719  227]\n",
            " [ 231 1223]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.76      0.76       946\n",
            "           1       0.84      0.84      0.84      1454\n",
            "\n",
            "    accuracy                           0.81      2400\n",
            "   macro avg       0.80      0.80      0.80      2400\n",
            "weighted avg       0.81      0.81      0.81      2400\n",
            "\n",
            "\n",
            "Training K-Nearest Neighbors...\n",
            "K-Nearest Neighbors Results:\n",
            "Training Accuracy: 0.3988\n",
            "Validation Accuracy: 0.3958\n",
            "Test Accuracy: 0.3942\n",
            "Test F1 Score (weighted): 0.2243\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 944    2]\n",
            " [1452    2]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      1.00      0.56       946\n",
            "           1       0.50      0.00      0.00      1454\n",
            "\n",
            "    accuracy                           0.39      2400\n",
            "   macro avg       0.45      0.50      0.28      2400\n",
            "weighted avg       0.46      0.39      0.22      2400\n",
            "\n",
            "\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:49:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Results:\n",
            "Training Accuracy: 0.9144\n",
            "Validation Accuracy: 0.8342\n",
            "Test Accuracy: 0.7983\n",
            "Test F1 Score (weighted): 0.7982\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 701  245]\n",
            " [ 239 1215]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.74      0.74       946\n",
            "           1       0.83      0.84      0.83      1454\n",
            "\n",
            "    accuracy                           0.80      2400\n",
            "   macro avg       0.79      0.79      0.79      2400\n",
            "weighted avg       0.80      0.80      0.80      2400\n",
            "\n",
            "\n",
            "Training LightGBM...\n",
            "[LightGBM] [Info] Number of positive: 5089, number of negative: 3311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107949 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 50647\n",
            "[LightGBM] [Info] Number of data points in the train set: 8400, number of used features: 1742\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.605833 -> initscore=0.429831\n",
            "[LightGBM] [Info] Start training from score 0.429831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM Results:\n",
            "Training Accuracy: 0.9050\n",
            "Validation Accuracy: 0.8242\n",
            "Test Accuracy: 0.8142\n",
            "Test F1 Score (weighted): 0.8142\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 725  221]\n",
            " [ 225 1229]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.77      0.76       946\n",
            "           1       0.85      0.85      0.85      1454\n",
            "\n",
            "    accuracy                           0.81      2400\n",
            "   macro avg       0.81      0.81      0.81      2400\n",
            "weighted avg       0.81      0.81      0.81      2400\n",
            "\n",
            "\n",
            "Models ranked by validation accuracy:\n",
            "1. XGBoost: 0.8342\n",
            "2. Random Forest: 0.8300\n",
            "3. LightGBM: 0.8242\n",
            "4. Support Vector Machine: 0.8233\n",
            "5. Multinomial NB: 0.7942\n",
            "6. K-Nearest Neighbors: 0.3958\n",
            "\n",
            "Training and evaluating models for all_labels classification...\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "Training Accuracy: 0.9995\n",
            "Validation Accuracy: 0.5833\n",
            "Test Accuracy: 0.5825\n",
            "Test F1 Score (weighted): 0.4964\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[887  41   0  14   0   0   4]\n",
            " [159 326   1  27   3   0   0]\n",
            " [130  97   2   3   0   1   2]\n",
            " [ 45  78   0 180   0   0   0]\n",
            " [ 25  42   1   2   0   0   3]\n",
            " [ 34  32   0   1   0   0   1]\n",
            " [197  56   1   0   2   0   3]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.94      0.73       946\n",
            "           1       0.49      0.63      0.55       516\n",
            "           2       0.40      0.01      0.02       235\n",
            "           3       0.79      0.59      0.68       303\n",
            "           4       0.00      0.00      0.00        73\n",
            "           5       0.00      0.00      0.00        68\n",
            "           6       0.23      0.01      0.02       259\n",
            "\n",
            "    accuracy                           0.58      2400\n",
            "   macro avg       0.36      0.31      0.29      2400\n",
            "weighted avg       0.51      0.58      0.50      2400\n",
            "\n",
            "\n",
            "Training Multinomial NB...\n",
            "Multinomial NB Results:\n",
            "Training Accuracy: 0.6244\n",
            "Validation Accuracy: 0.5875\n",
            "Test Accuracy: 0.5837\n",
            "Test F1 Score (weighted): 0.4960\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[882  52   0  10   0   0   2]\n",
            " [138 344   0  34   0   0   0]\n",
            " [119 109   0   7   0   0   0]\n",
            " [ 35  97   0 171   0   0   0]\n",
            " [ 24  47   0   2   0   0   0]\n",
            " [ 27  39   0   2   0   0   0]\n",
            " [188  67   0   0   0   0   4]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.93      0.75       946\n",
            "           1       0.46      0.67      0.54       516\n",
            "           2       0.00      0.00      0.00       235\n",
            "           3       0.76      0.56      0.65       303\n",
            "           4       0.00      0.00      0.00        73\n",
            "           5       0.00      0.00      0.00        68\n",
            "           6       0.67      0.02      0.03       259\n",
            "\n",
            "    accuracy                           0.58      2400\n",
            "   macro avg       0.36      0.31      0.28      2400\n",
            "weighted avg       0.51      0.58      0.50      2400\n",
            "\n",
            "\n",
            "Training Support Vector Machine...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Machine Results:\n",
            "Training Accuracy: 0.7668\n",
            "Validation Accuracy: 0.6000\n",
            "Test Accuracy: 0.5938\n",
            "Test F1 Score (weighted): 0.5349\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[857  56   7  12   0   0  14]\n",
            " [118 332  10  42   5   1   8]\n",
            " [103  98  18   4   0   0  12]\n",
            " [ 27  82   5 189   0   0   0]\n",
            " [ 16  41   5   3   4   0   4]\n",
            " [ 26  32   2   0   1   3   4]\n",
            " [167  49  14   3   2   2  22]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.91      0.76       946\n",
            "           1       0.48      0.64      0.55       516\n",
            "           2       0.30      0.08      0.12       235\n",
            "           3       0.75      0.62      0.68       303\n",
            "           4       0.33      0.05      0.09        73\n",
            "           5       0.50      0.04      0.08        68\n",
            "           6       0.34      0.08      0.14       259\n",
            "\n",
            "    accuracy                           0.59      2400\n",
            "   macro avg       0.48      0.35      0.35      2400\n",
            "weighted avg       0.55      0.59      0.53      2400\n",
            "\n",
            "\n",
            "Training K-Nearest Neighbors...\n",
            "K-Nearest Neighbors Results:\n",
            "Training Accuracy: 0.3959\n",
            "Validation Accuracy: 0.3942\n",
            "Test Accuracy: 0.3917\n",
            "Test F1 Score (weighted): 0.2221\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[940   5   0   0   0   0   1]\n",
            " [515   0   0   1   0   0   0]\n",
            " [233   2   0   0   0   0   0]\n",
            " [303   0   0   0   0   0   0]\n",
            " [ 73   0   0   0   0   0   0]\n",
            " [ 68   0   0   0   0   0   0]\n",
            " [259   0   0   0   0   0   0]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.99      0.56       946\n",
            "           1       0.00      0.00      0.00       516\n",
            "           2       0.00      0.00      0.00       235\n",
            "           3       0.00      0.00      0.00       303\n",
            "           4       0.00      0.00      0.00        73\n",
            "           5       0.00      0.00      0.00        68\n",
            "           6       0.00      0.00      0.00       259\n",
            "\n",
            "    accuracy                           0.39      2400\n",
            "   macro avg       0.06      0.14      0.08      2400\n",
            "weighted avg       0.15      0.39      0.22      2400\n",
            "\n",
            "\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:51:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Results:\n",
            "Training Accuracy: 0.8808\n",
            "Validation Accuracy: 0.5925\n",
            "Test Accuracy: 0.6004\n",
            "Test F1 Score (weighted): 0.5550\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[841  48  18  13   0   2  24]\n",
            " [102 321  16  49   7   8  13]\n",
            " [ 92  94  33   5   1   2   8]\n",
            " [ 26  67   4 204   1   0   1]\n",
            " [ 11  41   4   8   7   0   2]\n",
            " [ 19  32   4   1   0   9   3]\n",
            " [150  51  22   4   2   4  26]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.89      0.77       946\n",
            "           1       0.49      0.62      0.55       516\n",
            "           2       0.33      0.14      0.20       235\n",
            "           3       0.72      0.67      0.70       303\n",
            "           4       0.39      0.10      0.15        73\n",
            "           5       0.36      0.13      0.19        68\n",
            "           6       0.34      0.10      0.15       259\n",
            "\n",
            "    accuracy                           0.60      2400\n",
            "   macro avg       0.47      0.38      0.39      2400\n",
            "weighted avg       0.55      0.60      0.55      2400\n",
            "\n",
            "\n",
            "Training LightGBM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102166 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 50735\n",
            "[LightGBM] [Info] Number of data points in the train set: 8399, number of used features: 1732\n",
            "[LightGBM] [Info] Start training from score -0.930862\n",
            "[LightGBM] [Info] Start training from score -1.538106\n",
            "[LightGBM] [Info] Start training from score -2.321697\n",
            "[LightGBM] [Info] Start training from score -2.070788\n",
            "[LightGBM] [Info] Start training from score -3.486792\n",
            "[LightGBM] [Info] Start training from score -3.559404\n",
            "[LightGBM] [Info] Start training from score -2.227933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM Results:\n",
            "Training Accuracy: 0.9333\n",
            "Validation Accuracy: 0.5983\n",
            "Test Accuracy: 0.5946\n",
            "Test F1 Score (weighted): 0.5548\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[821  51  14  16   0   1  43]\n",
            " [ 97 314  21  51   9   8  16]\n",
            " [ 89  90  28  11   2   1  14]\n",
            " [ 17  63   3 212   1   0   7]\n",
            " [ 14  36   7   6   4   1   5]\n",
            " [ 16  29   4   3   0  12   4]\n",
            " [141  45  24   5   3   5  36]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.87      0.77       946\n",
            "           1       0.50      0.61      0.55       516\n",
            "           2       0.28      0.12      0.17       235\n",
            "           3       0.70      0.70      0.70       303\n",
            "           4       0.21      0.05      0.09        73\n",
            "           5       0.43      0.18      0.25        68\n",
            "           6       0.29      0.14      0.19       259\n",
            "\n",
            "    accuracy                           0.59      2400\n",
            "   macro avg       0.44      0.38      0.39      2400\n",
            "weighted avg       0.54      0.59      0.55      2400\n",
            "\n",
            "\n",
            "Models ranked by validation accuracy:\n",
            "1. Support Vector Machine: 0.6000\n",
            "2. LightGBM: 0.5983\n",
            "3. XGBoost: 0.5925\n",
            "4. Multinomial NB: 0.5875\n",
            "5. Random Forest: 0.5833\n",
            "6. K-Nearest Neighbors: 0.3942\n",
            "\n",
            "Training and evaluating models for levels classification...\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "Training Accuracy: 0.9996\n",
            "Validation Accuracy: 0.5833\n",
            "Test Accuracy: 0.5792\n",
            "Test F1 Score (weighted): 0.5107\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[  3  23 178   3  51]\n",
            " [  5  33 170  13 157]\n",
            " [  2   4 875  17  48]\n",
            " [  0   7  43 177  76]\n",
            " [  2  25 149  37 302]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.01      0.02       258\n",
            "           1       0.36      0.09      0.14       378\n",
            "           2       0.62      0.92      0.74       946\n",
            "           3       0.72      0.58      0.64       303\n",
            "           4       0.48      0.59      0.53       515\n",
            "\n",
            "    accuracy                           0.58      2400\n",
            "   macro avg       0.48      0.44      0.41      2400\n",
            "weighted avg       0.52      0.58      0.51      2400\n",
            "\n",
            "\n",
            "Training Multinomial NB...\n",
            "Multinomial NB Results:\n",
            "Training Accuracy: 0.6567\n",
            "Validation Accuracy: 0.5883\n",
            "Test Accuracy: 0.5800\n",
            "Test F1 Score (weighted): 0.5040\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[  1  25 177   5  50]\n",
            " [  0  20 164   8 186]\n",
            " [  1   5 887  14  39]\n",
            " [  0   4  34 171  94]\n",
            " [  0  16 146  40 313]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.00      0.01       258\n",
            "           1       0.29      0.05      0.09       378\n",
            "           2       0.63      0.94      0.75       946\n",
            "           3       0.72      0.56      0.63       303\n",
            "           4       0.46      0.61      0.52       515\n",
            "\n",
            "    accuracy                           0.58      2400\n",
            "   macro avg       0.52      0.43      0.40      2400\n",
            "weighted avg       0.54      0.58      0.50      2400\n",
            "\n",
            "\n",
            "Training Support Vector Machine...\n",
            "Support Vector Machine Results:\n",
            "Training Accuracy: 0.7911\n",
            "Validation Accuracy: 0.6117\n",
            "Test Accuracy: 0.5908\n",
            "Test F1 Score (weighted): 0.5588\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 21  48 145   2  42]\n",
            " [ 11  96 117  12 142]\n",
            " [ 24  41 825  13  43]\n",
            " [  2  14  20 194  73]\n",
            " [  6  86 102  39 282]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.08      0.13       258\n",
            "           1       0.34      0.25      0.29       378\n",
            "           2       0.68      0.87      0.77       946\n",
            "           3       0.75      0.64      0.69       303\n",
            "           4       0.48      0.55      0.51       515\n",
            "\n",
            "    accuracy                           0.59      2400\n",
            "   macro avg       0.52      0.48      0.48      2400\n",
            "weighted avg       0.56      0.59      0.56      2400\n",
            "\n",
            "\n",
            "Training K-Nearest Neighbors...\n",
            "K-Nearest Neighbors Results:\n",
            "Training Accuracy: 0.3962\n",
            "Validation Accuracy: 0.3942\n",
            "Test Accuracy: 0.3933\n",
            "Test F1 Score (weighted): 0.2227\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[  0   1 257   0   0]\n",
            " [  0   0 378   0   0]\n",
            " [  2   0 944   0   0]\n",
            " [  0   1 302   0   0]\n",
            " [  0   0 515   0   0]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       258\n",
            "           1       0.00      0.00      0.00       378\n",
            "           2       0.39      1.00      0.56       946\n",
            "           3       0.00      0.00      0.00       303\n",
            "           4       0.00      0.00      0.00       515\n",
            "\n",
            "    accuracy                           0.39      2400\n",
            "   macro avg       0.08      0.20      0.11      2400\n",
            "weighted avg       0.16      0.39      0.22      2400\n",
            "\n",
            "\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Results:\n",
            "Training Accuracy: 0.8780\n",
            "Validation Accuracy: 0.6125\n",
            "Test Accuracy: 0.5904\n",
            "Test F1 Score (weighted): 0.5585\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 20  54 137   6  41]\n",
            " [ 19  93 111  19 136]\n",
            " [ 26  39 823  12  46]\n",
            " [  3  19  26 198  57]\n",
            " [ 10  79  98  45 283]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.08      0.12       258\n",
            "           1       0.33      0.25      0.28       378\n",
            "           2       0.69      0.87      0.77       946\n",
            "           3       0.71      0.65      0.68       303\n",
            "           4       0.50      0.55      0.53       515\n",
            "\n",
            "    accuracy                           0.59      2400\n",
            "   macro avg       0.50      0.48      0.47      2400\n",
            "weighted avg       0.55      0.59      0.56      2400\n",
            "\n",
            "\n",
            "Training LightGBM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100359 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 49631\n",
            "[LightGBM] [Info] Number of data points in the train set: 8400, number of used features: 1714\n",
            "[LightGBM] [Info] Start training from score -2.226948\n",
            "[LightGBM] [Info] Start training from score -1.849843\n",
            "[LightGBM] [Info] Start training from score -0.930981\n",
            "[LightGBM] [Info] Start training from score -2.070907\n",
            "[LightGBM] [Info] Start training from score -1.538780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM Results:\n",
            "Training Accuracy: 0.9013\n",
            "Validation Accuracy: 0.6000\n",
            "Test Accuracy: 0.5904\n",
            "Test F1 Score (weighted): 0.5693\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 36  49 121   8  44]\n",
            " [ 21 111  86  21 139]\n",
            " [ 34  42 799  17  54]\n",
            " [  3  20  27 191  62]\n",
            " [ 14  91  77  53 280]]\n",
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.14      0.20       258\n",
            "           1       0.35      0.29      0.32       378\n",
            "           2       0.72      0.84      0.78       946\n",
            "           3       0.66      0.63      0.64       303\n",
            "           4       0.48      0.54      0.51       515\n",
            "\n",
            "    accuracy                           0.59      2400\n",
            "   macro avg       0.51      0.49      0.49      2400\n",
            "weighted avg       0.56      0.59      0.57      2400\n",
            "\n",
            "\n",
            "Models ranked by validation accuracy:\n",
            "1. XGBoost: 0.6125\n",
            "2. Support Vector Machine: 0.6117\n",
            "3. LightGBM: 0.6000\n",
            "4. Multinomial NB: 0.5883\n",
            "5. Random Forest: 0.5833\n",
            "6. K-Nearest Neighbors: 0.3942\n",
            "\n",
            "Saving the best model and encoders...\n",
            "Best model: XGBoost with validation accuracy: 0.8342\n",
            "Model and encoders for binary saved successfully!\n",
            "\n",
            "Saving the best model and encoders...\n",
            "Best model: Support Vector Machine with validation accuracy: 0.6000\n",
            "Model and encoders for all_labels saved successfully!\n",
            "\n",
            "Saving the best model and encoders...\n",
            "Best model: XGBoost with validation accuracy: 0.6125\n",
            "Model and encoders for levels saved successfully!\n",
            "\n",
            "All models trained successfully!\n",
            "\n",
            "================================================================================\n",
            "URDU ANXIETY PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "Enter Urdu text to predict anxiety level (or type 'exit' to quit):\n",
            ">> میں اتنا برا تھا کہ اس کا بیان نہیں کر سکتا، لیکن اب میں اتنا اچھا ہوں... میں وہ سب کر سکتا ہوں جو کوئی اور نہیں کر سکتا\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS FOR USER INPUT TEXT\n",
            "================================================================================\n",
            "\n",
            "Original Text: میں اتنا برا تھا کہ اس کا بیان نہیں کر سکتا، لیکن اب میں اتنا اچھا ہوں... میں وہ سب کر سکتا ہوں جو کوئی اور نہیں کر سکتا\n",
            "Preprocessed Text: اتنا برا بیان نہیں سکتا اتنا اچھا سب سکتا کوئی نہیں سکتا\n",
            "\n",
            "1. ANXIETY PREDICTION (Binary)\n",
            "------------------------------\n",
            "Prediction: Not Anxious\n",
            "Confidence: 70.61%\n",
            "\n",
            "2. ALL LABELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: ['agoraphobia']\n",
            "Confidence: 53.31%\n",
            "\n",
            "3. LEVELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: No Anxiety\n",
            "Confidence: 72.09%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Do you want to make another prediction? (y/n): y\n",
            "\n",
            "================================================================================\n",
            "URDU ANXIETY PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "Enter Urdu text to predict anxiety level (or type 'exit' to quit):\n",
            ">> میں نے اپنے والدین کو اپنے مسائل کے بارے میں بتایا۔ اور وہ بار بار کہتے رہے کہ میں مبالغہ آرائی کر رہا ہوں۔ مجھے نہیں لگتا کہ میں اس کا حق دار ہوں۔\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS FOR USER INPUT TEXT\n",
            "================================================================================\n",
            "\n",
            "Original Text: میں نے اپنے والدین کو اپنے مسائل کے بارے میں بتایا۔ اور وہ بار بار کہتے رہے کہ میں مبالغہ آرائی کر رہا ہوں۔ مجھے نہیں لگتا کہ میں اس کا حق دار ہوں۔\n",
            "Preprocessed Text: اپنے والدین اپنے مسائل بارے بتایا۔ بار بار کہتے مبالغہ آرائی ہوں۔ مجھے نہیں لگتا حق دار ہوں۔\n",
            "\n",
            "1. ANXIETY PREDICTION (Binary)\n",
            "------------------------------\n",
            "Prediction: Anxious\n",
            "Confidence: 78.24%\n",
            "\n",
            "2. ALL LABELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: ['general']\n",
            "Confidence: 33.29%\n",
            "\n",
            "3. LEVELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: Moderate Anxiety\n",
            "Confidence: 35.33%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Do you want to make another prediction? (y/n): y\n",
            "\n",
            "================================================================================\n",
            "URDU ANXIETY PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "Enter Urdu text to predict anxiety level (or type 'exit' to quit):\n",
            ">> مجھے امتحان کی تھوڑی سی ٹینشن ہے، لیکن میں تیار ہوں\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS FOR USER INPUT TEXT\n",
            "================================================================================\n",
            "\n",
            "Original Text: مجھے امتحان کی تھوڑی سی ٹینشن ہے، لیکن میں تیار ہوں\n",
            "Preprocessed Text: مجھے امتحان تھوڑی سی ٹینشن تیار\n",
            "\n",
            "1. ANXIETY PREDICTION (Binary)\n",
            "------------------------------\n",
            "Prediction: Anxious\n",
            "Confidence: 50.29%\n",
            "\n",
            "2. ALL LABELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: ['socialanxiety']\n",
            "Confidence: 45.59%\n",
            "\n",
            "3. LEVELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: No Anxiety\n",
            "Confidence: 40.06%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Do you want to make another prediction? (y/n): y\n",
            "\n",
            "================================================================================\n",
            "URDU ANXIETY PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "Enter Urdu text to predict anxiety level (or type 'exit' to quit):\n",
            ">> آج کا دن بہت اچھا رہا، سب کچھ ٹھیک تھا۔\t\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS FOR USER INPUT TEXT\n",
            "================================================================================\n",
            "\n",
            "Original Text: آج کا دن بہت اچھا رہا، سب کچھ ٹھیک تھا۔\t\n",
            "Preprocessed Text: آج دن اچھا سب کچھ ٹھیک تھا۔\n",
            "\n",
            "1. ANXIETY PREDICTION (Binary)\n",
            "------------------------------\n",
            "Prediction: Not Anxious\n",
            "Confidence: 77.54%\n",
            "\n",
            "2. ALL LABELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: Normal\n",
            "Confidence: 50.79%\n",
            "\n",
            "3. LEVELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: No Anxiety\n",
            "Confidence: 84.62%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Do you want to make another prediction? (y/n): y\n",
            "\n",
            "================================================================================\n",
            "URDU ANXIETY PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "Enter Urdu text to predict anxiety level (or type 'exit' to quit):\n",
            ">> تھوڑا سا دباؤ ہے لیکن سنبھالا جا سکتا ہے۔\t\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS FOR USER INPUT TEXT\n",
            "================================================================================\n",
            "\n",
            "Original Text: تھوڑا سا دباؤ ہے لیکن سنبھالا جا سکتا ہے۔\t\n",
            "Preprocessed Text: تھوڑا سا دباؤ سنبھالا جا سکتا ہے۔\n",
            "\n",
            "1. ANXIETY PREDICTION (Binary)\n",
            "------------------------------\n",
            "Prediction: Not Anxious\n",
            "Confidence: 79.47%\n",
            "\n",
            "2. ALL LABELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: Normal\n",
            "Confidence: 64.85%\n",
            "\n",
            "3. LEVELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: No Anxiety\n",
            "Confidence: 76.91%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Do you want to make another prediction? (y/n): y\n",
            "\n",
            "================================================================================\n",
            "URDU ANXIETY PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "Enter Urdu text to predict anxiety level (or type 'exit' to quit):\n",
            ">> دل بار بار ڈر رہا ہے کہ کچھ غلط ہونے والا ہے۔\t\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS FOR USER INPUT TEXT\n",
            "================================================================================\n",
            "\n",
            "Original Text: دل بار بار ڈر رہا ہے کہ کچھ غلط ہونے والا ہے۔\t\n",
            "Preprocessed Text: دل بار بار ڈر کچھ غلط ہونے ہے۔\n",
            "\n",
            "1. ANXIETY PREDICTION (Binary)\n",
            "------------------------------\n",
            "Prediction: Anxious\n",
            "Confidence: 85.36%\n",
            "\n",
            "2. ALL LABELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: ['socialanxiety']\n",
            "Confidence: 32.90%\n",
            "\n",
            "3. LEVELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: Mild Anxiety\n",
            "Confidence: 33.77%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Do you want to make another prediction? (y/n): y\n",
            "\n",
            "================================================================================\n",
            "URDU ANXIETY PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "Enter Urdu text to predict anxiety level (or type 'exit' to quit):\n",
            ">> میں آج بہت پریشان ہوں، دل گھبرا رہا ہے اور سانس لینا مشکل ہو رہا ہے۔\t\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS FOR USER INPUT TEXT\n",
            "================================================================================\n",
            "\n",
            "Original Text: میں آج بہت پریشان ہوں، دل گھبرا رہا ہے اور سانس لینا مشکل ہو رہا ہے۔\t\n",
            "Preprocessed Text: آج پریشان دل گھبرا سانس لینا مشکل ہو ہے۔\n",
            "\n",
            "1. ANXIETY PREDICTION (Binary)\n",
            "------------------------------\n",
            "Prediction: Anxious\n",
            "Confidence: 99.01%\n",
            "\n",
            "2. ALL LABELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: ['agoraphobia']\n",
            "Confidence: 32.71%\n",
            "\n",
            "3. LEVELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: Panic Anxiety\n",
            "Confidence: 42.44%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Do you want to make another prediction? (y/n): y\n",
            "\n",
            "================================================================================\n",
            "URDU ANXIETY PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "Enter Urdu text to predict anxiety level (or type 'exit' to quit):\n",
            ">> مجھے اپنے پیر کے لیے ایکس رے کے لیے ہسپتال جانا پڑ سکتا ہے۔ ڈاکٹر کے کال بیک کا انتظار کرنا ہوگا تاکہ تصدیق ہو سکے۔ مجھے ہسپتال سے نفرت ہے۔\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS FOR USER INPUT TEXT\n",
            "================================================================================\n",
            "\n",
            "Original Text: مجھے اپنے پیر کے لیے ایکس رے کے لیے ہسپتال جانا پڑ سکتا ہے۔ ڈاکٹر کے کال بیک کا انتظار کرنا ہوگا تاکہ تصدیق ہو سکے۔ مجھے ہسپتال سے نفرت ہے۔\n",
            "Preprocessed Text: مجھے اپنے پیر ایکس رے ہسپتال جانا پڑ سکتا ہے۔ ڈاکٹر کال بیک انتظار کر تصدیق ہو سکے۔ مجھے ہسپتال نفرت ہے۔\n",
            "\n",
            "1. ANXIETY PREDICTION (Binary)\n",
            "------------------------------\n",
            "Prediction: Anxious\n",
            "Confidence: 59.88%\n",
            "\n",
            "2. ALL LABELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: Normal\n",
            "Confidence: 51.25%\n",
            "\n",
            "3. LEVELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: Mild Anxiety\n",
            "Confidence: 49.13%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Do you want to make another prediction? (y/n): y\n",
            "\n",
            "================================================================================\n",
            "URDU ANXIETY PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "Enter Urdu text to predict anxiety level (or type 'exit' to quit):\n",
            ">> سب کو شب بخیر اور جیرڈ، کبھی بھی کائٹ پف میں چیٹ نہ کریں۔\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS FOR USER INPUT TEXT\n",
            "================================================================================\n",
            "\n",
            "Original Text: سب کو شب بخیر اور جیرڈ، کبھی بھی کائٹ پف میں چیٹ نہ کریں۔\n",
            "Preprocessed Text: سب شب بخیر جیرڈ کبھی کائٹ پف چیٹ نہ کریں۔\n",
            "\n",
            "1. ANXIETY PREDICTION (Binary)\n",
            "------------------------------\n",
            "Prediction: Not Anxious\n",
            "Confidence: 88.38%\n",
            "\n",
            "2. ALL LABELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: Normal\n",
            "Confidence: 89.75%\n",
            "\n",
            "3. LEVELS PREDICTION\n",
            "------------------------------\n",
            "Prediction: No Anxiety\n",
            "Confidence: 90.40%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Do you want to make another prediction? (y/n): n\n",
            "Thank you for using the Urdu Anxiety Prediction System!\n"
          ]
        }
      ]
    }
  ]
}